{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CaptFormer_Exp_1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOrp7siJ88b5YCosSu0YRU1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/parmarsuraj99/keras-transformer-flex/blob/master/VirTex/CaptFormer_Exp_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mD3ntzOyskf8",
        "colab_type": "text"
      },
      "source": [
        "[VirTex - Learning Visual Representations from Textual Annotations](https://github.com/kdexd/virtex)\n",
        "\n",
        "Karan Desai and Justin Johnson\n",
        "University of Michigan\n",
        "\n",
        "Preprint: [arxiv.org/abs/2006.06666](https://arxiv.org/abs/2006.06666)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ik3OucwskeE",
        "colab_type": "text"
      },
      "source": [
        "### installling HuggingFace Transformers to use a pretrained tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G_TfT3UnFKnl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 661
        },
        "outputId": "9281a2d9-3644-4dee-e287-4b1cf42d7bd2"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\r\u001b[K     |▍                               | 10kB 27.0MB/s eta 0:00:01\r\u001b[K     |▉                               | 20kB 3.4MB/s eta 0:00:01\r\u001b[K     |█▎                              | 30kB 4.5MB/s eta 0:00:01\r\u001b[K     |█▊                              | 40kB 4.8MB/s eta 0:00:01\r\u001b[K     |██▏                             | 51kB 4.0MB/s eta 0:00:01\r\u001b[K     |██▋                             | 61kB 4.5MB/s eta 0:00:01\r\u001b[K     |███                             | 71kB 4.8MB/s eta 0:00:01\r\u001b[K     |███▍                            | 81kB 5.2MB/s eta 0:00:01\r\u001b[K     |███▉                            | 92kB 5.5MB/s eta 0:00:01\r\u001b[K     |████▎                           | 102kB 5.4MB/s eta 0:00:01\r\u001b[K     |████▊                           | 112kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 122kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 133kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████                          | 143kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 153kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████▉                         | 163kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 174kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 184kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 194kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 204kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████                       | 215kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 225kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 235kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 245kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 256kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████                     | 266kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 276kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████                    | 286kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 296kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 307kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 317kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 327kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 337kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 348kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 358kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 368kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 378kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 389kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 399kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 409kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 419kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 430kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 440kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 450kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▏            | 460kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 471kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 481kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 491kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 501kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 512kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 522kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 532kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 542kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 552kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 563kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 573kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 583kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 593kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 604kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 614kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 624kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 634kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 645kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 655kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 665kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 675kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 686kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 696kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 706kB 5.4MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 716kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 727kB 5.4MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 737kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 747kB 5.4MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 757kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 768kB 5.4MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 778kB 5.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\r\u001b[K     |▍                               | 10kB 25.5MB/s eta 0:00:01\r\u001b[K     |▊                               | 20kB 33.3MB/s eta 0:00:01\r\u001b[K     |█▏                              | 30kB 39.3MB/s eta 0:00:01\r\u001b[K     |█▌                              | 40kB 32.9MB/s eta 0:00:01\r\u001b[K     |█▉                              | 51kB 35.4MB/s eta 0:00:01\r\u001b[K     |██▎                             | 61kB 38.7MB/s eta 0:00:01\r\u001b[K     |██▋                             | 71kB 32.8MB/s eta 0:00:01\r\u001b[K     |███                             | 81kB 34.3MB/s eta 0:00:01\r\u001b[K     |███▍                            | 92kB 29.0MB/s eta 0:00:01\r\u001b[K     |███▊                            | 102kB 30.5MB/s eta 0:00:01\r\u001b[K     |████                            | 112kB 30.5MB/s eta 0:00:01\r\u001b[K     |████▌                           | 122kB 30.5MB/s eta 0:00:01\r\u001b[K     |████▉                           | 133kB 30.5MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 143kB 30.5MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 153kB 30.5MB/s eta 0:00:01\r\u001b[K     |██████                          | 163kB 30.5MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 174kB 30.5MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 184kB 30.5MB/s eta 0:00:01\r\u001b[K     |███████                         | 194kB 30.5MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 204kB 30.5MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 215kB 30.5MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 225kB 30.5MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 235kB 30.5MB/s eta 0:00:01\r\u001b[K     |█████████                       | 245kB 30.5MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 256kB 30.5MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 266kB 30.5MB/s eta 0:00:01\r\u001b[K     |██████████                      | 276kB 30.5MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 286kB 30.5MB/s eta 0:00:01\r\u001b[K     |██████████▊                     | 296kB 30.5MB/s eta 0:00:01\r\u001b[K     |███████████▏                    | 307kB 30.5MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 317kB 30.5MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 327kB 30.5MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 337kB 30.5MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 348kB 30.5MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 358kB 30.5MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 368kB 30.5MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 378kB 30.5MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 389kB 30.5MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 399kB 30.5MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 409kB 30.5MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 419kB 30.5MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 430kB 30.5MB/s eta 0:00:01\r\u001b[K     |████████████████                | 440kB 30.5MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 450kB 30.5MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 460kB 30.5MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 471kB 30.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 481kB 30.5MB/s eta 0:00:01\r\u001b[K     |█████████████████▉              | 491kB 30.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 501kB 30.5MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 512kB 30.5MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 522kB 30.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 532kB 30.5MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 542kB 30.5MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 552kB 30.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 563kB 30.5MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 573kB 30.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 583kB 30.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 593kB 30.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 604kB 30.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 614kB 30.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 624kB 30.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 634kB 30.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 645kB 30.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 655kB 30.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 665kB 30.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 675kB 30.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 686kB 30.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 696kB 30.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 706kB 30.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 716kB 30.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 727kB 30.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 737kB 30.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 747kB 30.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 757kB 30.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 768kB 30.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 778kB 30.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 788kB 30.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 798kB 30.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 808kB 30.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 819kB 30.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 829kB 30.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 839kB 30.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 849kB 30.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 860kB 30.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 870kB 30.5MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 880kB 30.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 890kB 30.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 42.3MB/s \n",
            "\u001b[?25hCollecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 55.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.12.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.15.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=cd4ec5543140706c162b7f1afa0f2d78bb9100ac49f637bf6a58630247dada87\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sacremoses, sentencepiece, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L72ZRbyZt2bm",
        "colab_type": "text"
      },
      "source": [
        "### importing required TF stuff and a backbone(ResNet50)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ym2rsKNGnVBB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gc\n",
        "import functools\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow.keras.layers as L\n",
        "from tensorflow.keras.layers import Input\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array, array_to_img"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fYIL4KMi8lE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "d3946a35-33c6-4c5c-c10d-7ef1eed950b6"
      },
      "source": [
        "visual_backbone = tf.keras.applications.ResNet50(input_shape=(224, 224, 3), include_top=False, weights=\"imagenet\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94773248/94765736 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNvaZgEXt-Y-",
        "colab_type": "text"
      },
      "source": [
        "##Transformer in keras\n",
        "\n",
        "Heavily inspired by this TensorFlow Example [Transformer model for language understanding](https://www.tensorflow.org/tutorials/text/transformer)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LIs5lpW57seJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scaled_dot_product_attention(query, key, value, mask):\n",
        "  \"\"\"Calculate the attention weights. \"\"\"\n",
        "  matmul_qk = tf.matmul(query, key, transpose_b=True)\n",
        "\n",
        "  # scale matmul_qk\n",
        "  depth = tf.cast(tf.shape(key)[-1], tf.float32)\n",
        "  logits = matmul_qk / tf.math.sqrt(depth)\n",
        "\n",
        "  # add the mask to zero out padding tokens\n",
        "  if mask is not None:\n",
        "    logits += (mask * -1e9)\n",
        "\n",
        "  # softmax is normalized on the last axis (seq_len_k)\n",
        "  attention_weights = tf.nn.softmax(logits, axis=-1)\n",
        "\n",
        "  output = tf.matmul(attention_weights, value)\n",
        "\n",
        "  return output"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VLhpLH77scR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiHeadAttention(tf.keras.layers.Layer):\n",
        "\n",
        "  def __init__(self, d_model, num_heads, name=\"multi_head_attention\"):\n",
        "    super(MultiHeadAttention, self).__init__(name=name)\n",
        "    self.num_heads = num_heads\n",
        "    self.d_model = d_model\n",
        "\n",
        "    assert d_model % self.num_heads == 0\n",
        "\n",
        "    self.depth = d_model // self.num_heads\n",
        "\n",
        "    self.query_dense = tf.keras.layers.Dense(units=d_model)\n",
        "    self.key_dense = tf.keras.layers.Dense(units=d_model)\n",
        "    self.value_dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "    self.dense = tf.keras.layers.Dense(units=d_model)\n",
        "\n",
        "  def split_heads(self, inputs, batch_size):\n",
        "    inputs = tf.reshape(\n",
        "        inputs, shape=(batch_size, -1, self.num_heads, self.depth))\n",
        "    return tf.transpose(inputs, perm=[0, 2, 1, 3])\n",
        "\n",
        "  def call(self, query, key, value, mask):\n",
        "    batch_size = tf.shape(query)[0]\n",
        "\n",
        "    # linear layers\n",
        "    query = self.query_dense(query)\n",
        "    key = self.key_dense(key)\n",
        "    value = self.value_dense(value)\n",
        "\n",
        "    # split heads\n",
        "    query = self.split_heads(query, batch_size)\n",
        "    key = self.split_heads(key, batch_size)\n",
        "    value = self.split_heads(value, batch_size)\n",
        "\n",
        "    # scaled dot-product attention\n",
        "    print(f\"Q:{query.shape}, K:{key.shape}, V:{value.shape}\")\n",
        "    scaled_attention = scaled_dot_product_attention(query, key, value, mask)\n",
        "\n",
        "    scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
        "\n",
        "    # concatenation of heads\n",
        "    concat_attention = tf.reshape(scaled_attention,\n",
        "                                  (batch_size, -1, self.d_model))\n",
        "\n",
        "    # final linear layer\n",
        "    outputs = self.dense(concat_attention)\n",
        "\n",
        "    return outputs"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tSx0L5gJ9M9U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "391533fe-fa5f-46fe-d615-ccdbd5ee2943"
      },
      "source": [
        "#Teting MHA\n",
        "\n",
        "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
        "y = tf.random.uniform((20, 30, 512))  # (batch_size, encoder_sequence, d_model)\n",
        "y2 = tf.random.uniform((20, 32, 512))  # (batch_size, encoder_sequence, d_model)\n",
        "out = temp_mha(y2, y, y, mask=None)\n",
        "out.shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Q:(20, 8, 32, 64), K:(20, 8, 30, 64), V:(20, 8, 30, 64)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([20, 32, 512])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jHPq2O9v7saM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def point_wise_feed_forward_network(d_model, dff):\n",
        "  return tf.keras.Sequential([\n",
        "      tf.keras.layers.Dense(dff, activation='relu'),  # (batch_size, seq_len, dff)\n",
        "      tf.keras.layers.Dense(d_model)  # (batch_size, seq_len, d_model)\n",
        "  ])\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfjG8L_Lueps",
        "colab_type": "text"
      },
      "source": [
        "**We'll use Decoder layer only**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSRpZHtlwAxS",
        "colab_type": "text"
      },
      "source": [
        "**Note**\n",
        "LayerNormalization is applied before Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHSnEUVo7zxD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DecoderLayer(tf.keras.layers.Layer):\n",
        "  def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
        "    super(DecoderLayer, self).__init__()\n",
        "\n",
        "    self.self_attn = MultiHeadAttention(d_model = d_model, num_heads=num_heads)\n",
        "    self.mha = MultiHeadAttention(d_model=d_model, num_heads=num_heads)\n",
        "\n",
        "    self.ffn1 = tf.keras.layers.Dense(dff, activation='relu')\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    self.ffn2 = tf.keras.layers.Dense(d_model)\n",
        "\n",
        "    self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "    \n",
        "    self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "    self.dropout3 = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "    \n",
        "  def call(self, tgt, enc_output, training=True,\n",
        "           look_ahead_mask=None, padding_mask=None):\n",
        "      \n",
        "    tgt = tf.transpose(tgt, [1, 0, 2])\n",
        "    enc_output = tf.transpose(enc_output, [1, 0, 2])\n",
        "      \n",
        "    #Changed First layernorm then  masked attn\n",
        "    tgt = self.layernorm1(tgt)\n",
        "    print(f\"TGT:{tgt.shape}\")\n",
        "    tgt2 = self.self_attn(tgt, tgt, tgt, mask=look_ahead_mask)\n",
        "    #print(tgt2.shape)\n",
        "    tgt = tgt + self.dropout1(tgt2)\n",
        "    \n",
        "    #print(enc_output.shape)\n",
        "    #LayerNorm then decoder attn\n",
        "    tgt = self.layernorm2(tgt)\n",
        "    print(f\"TGT:{tgt.shape}, ENC: {enc_output.shape}\")\n",
        "    tgt2 = self.mha(tgt, enc_output, enc_output, mask=None)\n",
        "    print(f\"target: {tgt.shape}, enc_op:{enc_output.shape}\")\n",
        "    tgt = tgt + self.dropout2(tgt2)\n",
        "    \n",
        "    #LayerNorm then FFN\n",
        "    tgt = self.layernorm3(tgt)\n",
        "    tgt2 = self.ffn2(self.dropout(self.ffn1(tgt),training))\n",
        "    tgt = tgt + self.dropout3(tgt2)\n",
        "    \n",
        "    tgt = tf.transpose(tgt, [1, 0, 2])\n",
        "\n",
        "    return tgt"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V1UqTZIo78eF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.layers.Layer):\n",
        "  def __init__(self, num_layers, d_model, num_heads, dff, rate=0.1):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.d_model = d_model\n",
        "    self.num_layers = num_layers\n",
        "\n",
        "    self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) \n",
        "                       for _ in range(num_layers)]\n",
        "    self.dropout = tf.keras.layers.Dropout(rate)\n",
        "    \n",
        "  def call(self, x, enc_output, training, \n",
        "           look_ahead_mask, padding_mask):\n",
        "\n",
        "    attention_weights = {}\n",
        "\n",
        "    for i in range(self.num_layers):\n",
        "      x = self.dec_layers[i](x, enc_output, training,\n",
        "                                             look_ahead_mask, padding_mask)\n",
        "    \n",
        "    # x.shape == (batch_size, target_seq_len, d_model)\n",
        "    return x\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9eC8EXUNtkx8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "outputId": "ce2ed687-5667-4243-f653-32864f5235e6"
      },
      "source": [
        "%%time\n",
        "sample_decoder = Decoder(num_layers=2, d_model=512, num_heads=8, \n",
        "                         dff=2048)\n",
        "\n",
        "tmp_memory = tf.random.uniform((60, 64, 512), dtype=tf.float32, minval=0, maxval=200)\n",
        "tmp_tgt = tf.random.uniform((62, 64, 512), dtype=tf.float32, minval=0, maxval=200)\n",
        "\n",
        "tgt_mask = tf.linalg.band_part(tf.ones([62, 62]), 0, -1)\n",
        "\n",
        "output = sample_decoder(tmp_tgt, \n",
        "                        enc_output=tmp_memory, \n",
        "                        training=False,\n",
        "                        look_ahead_mask=tgt_mask, \n",
        "                        padding_mask=None\n",
        "                    )\n",
        "\n",
        "output.shape"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TGT:(64, 62, 512)\n",
            "Q:(64, 8, 62, 64), K:(64, 8, 62, 64), V:(64, 8, 62, 64)\n",
            "TGT:(64, 62, 512), ENC: (64, 60, 512)\n",
            "Q:(64, 8, 62, 64), K:(64, 8, 60, 64), V:(64, 8, 60, 64)\n",
            "target: (64, 62, 512), enc_op:(64, 60, 512)\n",
            "TGT:(64, 62, 512)\n",
            "Q:(64, 8, 62, 64), K:(64, 8, 62, 64), V:(64, 8, 62, 64)\n",
            "TGT:(64, 62, 512), ENC: (64, 60, 512)\n",
            "Q:(64, 8, 62, 64), K:(64, 8, 60, 64), V:(64, 8, 60, 64)\n",
            "target: (64, 62, 512), enc_op:(64, 60, 512)\n",
            "CPU times: user 124 ms, sys: 7.5 ms, total: 132 ms\n",
            "Wall time: 209 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uoPmNwIo3cZt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class WordAndPositionalEmbedding(tf.keras.layers.Layer):\n",
        "    r\"\"\"\n",
        "    A :class:`~torch.nn.Module` for learned word embeddings and position\n",
        "    embeddings for input tokens. Each token is mapped to a fixed dimensional\n",
        "    word embedding; and corresponding positional embedding based on its index.\n",
        "    These are summed together followed by layer normalization and an optional\n",
        "    dropout.\n",
        "    Parameters\n",
        "    ----------\n",
        "    vocab_size: int\n",
        "        Size of token vocabulary.\n",
        "    hidden_size: int\n",
        "        Size of token embedding vectors.\n",
        "    max_caption_length: int, optional (default = 30)\n",
        "        Maximum length of input captions; this is used to create a fixed\n",
        "        positional embedding lookup table.\n",
        "    dropout: float, optional (default = 0.1)\n",
        "        Dropout probability for final dropout applied after layer normalization.\n",
        "    padding_idx: int, optional (default = 0)\n",
        "        Token index of ``[PAD]`` token, word embedding for these tokens will\n",
        "        be a vector of zeroes (and not trainable).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, \n",
        "                 vocab_size: int, \n",
        "                 hidden_size:int, \n",
        "                 max_caption_length: int = 30, \n",
        "                 rate:float = 0.0,\n",
        "                 padding_idx: int=0):\n",
        "        super(WordAndPositionalEmbedding, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.padding_idx = padding_idx\n",
        "\n",
        "        self.words = L.Embedding(vocab_size, hidden_size, mask_zero=True)\n",
        "\n",
        "        self.positions = L.Embedding(max_caption_length, hidden_size)\n",
        "        self.layer_norm = L.LayerNormalization(\n",
        "            epsilon=1e-8,\n",
        "        )\n",
        "\n",
        "        self.dropout = L.Dropout(rate = rate)\n",
        "\n",
        "\n",
        "    def call(self, tokens):\n",
        "        \n",
        "        r\"\"\"\n",
        "        Get combined word and positional embeddings for input tokens.\n",
        "        Parameters\n",
        "        ----------\n",
        "        tokens: torch.Tensor\n",
        "            A tensor of shape ``(batch_size, max_caption_length)`` containing\n",
        "            a batch of caption tokens, with values in ``[0, vocab_size)``.\n",
        "        Returns\n",
        "        -------\n",
        "        torch.Tensor\n",
        "            A tensor of shape ``(batch_size, max_caption_length, hidden_size)``\n",
        "            containing corresponding token embeddings.\n",
        "        \"\"\"\n",
        "        print(tokens.shape)\n",
        "        position_indices = self._create_position_indices(tokens)\n",
        "        print(position_indices.shape)\n",
        "\n",
        "        word_embeddings = self.words(tokens)\n",
        "        positional_embeddings = self.positions(position_indices)\n",
        "\n",
        "\n",
        "        embeddings = self.layer_norm(word_embeddings + positional_embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "\n",
        "\n",
        "        token_mask = tf.expand_dims(tokens != self.padding_idx, -1)\n",
        "\n",
        "\n",
        "        embeddings = embeddings * tf.cast(token_mask, dtype=embeddings.dtype)\n",
        "        return embeddings\n",
        "\n",
        "    def _create_position_indices(self, tokens):\n",
        "\n",
        "        # Create position indices of the same size as token indices.\n",
        "        batch_size, max_caption_length = tokens.shape\n",
        "        positions = tf.range(\n",
        "            max_caption_length, dtype=tokens.dtype\n",
        "        )\n",
        "        # shape: (batch_size, max_caption_length)\n",
        "        positions = tf.broadcast_to(tf.expand_dims(positions, 0), [batch_size, max_caption_length])        \n",
        "        return positions"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXlOvFRlMT25",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextualHead(tf.keras.layers.Layer):\n",
        "    r\"\"\"\n",
        "    Base class for all textual heads. All child classes can simply inherit\n",
        "    from :class:`~torch.nn.Module`, however this is kept here for uniform\n",
        "    type annotations.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        hidden_size: int,\n",
        "    ):\n",
        "        super(TextualHead, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "    @property\n",
        "    def textual_feature_size(self):\n",
        "        \n",
        "        r\"\"\"\n",
        "        Size of the last dimension of output from forward pass; typically same\n",
        "        as :attr:`hidden_size` for most modules. This property is used to add\n",
        "        more modules on top of this.\n",
        "        \"\"\"\n",
        "        return self.hidden_size\n",
        "\n",
        "\n",
        "class LinearTextualHead(TextualHead):\n",
        "    r\"\"\"\n",
        "    Textual head containing a single linear layer projecting from textual\n",
        "    feature size to output vocabulary size.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        hidden_size: int,\n",
        "    ):\n",
        "        super(LinearTextualHead, self).__init__(vocab_size, hidden_Size)\n",
        "        self.output = L.Dense(vocab_size)\n",
        "\n",
        "    def call(self,\n",
        "             caption_tokens,\n",
        "             caption_lengths,\n",
        "             visual_features):\n",
        "        \n",
        "        output_logits = self.output(visual_features)\n",
        "        return output_logits\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_w79BokS-tK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TransformerTextualHead(TextualHead):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,\n",
        "        hidden_size:int,\n",
        "        num_layers: int,\n",
        "        attention_heads: int,\n",
        "        feedforward_size: int,\n",
        "        dropout: float = 0.1,\n",
        "        norm_type:str=\"pre\",\n",
        "        padding_idx: int=0,\n",
        "        max_caption_length: int = 30\n",
        "    ):\n",
        "        super().__init__(vocab_size, hidden_size)\n",
        "        self.num_layers = num_layers\n",
        "        self.attention_heads =attention_heads\n",
        "        self.feedforward_size = feedforward_size\n",
        "        self.dropout = dropout\n",
        "        self.padding_idx = padding_idx\n",
        "\n",
        "        self.embedding = WordAndPositionalEmbedding(\n",
        "            self.vocab_size,\n",
        "            self.textual_feature_size,\n",
        "            max_caption_length = max_caption_length,\n",
        "            rate = dropout,\n",
        "        )\n",
        "        LayerClass = (\n",
        "            DecoderLayer\n",
        "        )\n",
        "        _layer = DecoderLayer(\n",
        "            self.textual_feature_size,\n",
        "            self.attention_heads,\n",
        "            dff = self.feedforward_size,\n",
        "            rate = dropout\n",
        "        )\n",
        "        \n",
        "        self.encoder = Decoder(self.num_layers, self.textual_feature_size,\n",
        "            self.attention_heads,\n",
        "            dff = self.feedforward_size,\n",
        "            rate = dropout)\n",
        "        \"\"\"\n",
        "        self.encoder = DecoderLayer(\n",
        "            self.textual_feature_size,\n",
        "            self.attention_heads,\n",
        "            dff = self.feedforward_size,\n",
        "            rate = dropout)\n",
        "        \"\"\"\n",
        "\n",
        "        self.outputL = L.Dense(vocab_size)\n",
        "        #self.output.weight = self.embedding.words.weight\n",
        "\n",
        "    def call(self,\n",
        "             caption_tokens,\n",
        "             caption_lengths,\n",
        "             visual_features\n",
        "             ):\n",
        "        batch_size, max_caption_length = caption_tokens.shape\n",
        "        print(max_caption_length)\n",
        "\n",
        "        ones = tf.ones_like(caption_tokens)\n",
        "        caption_mask = tf.expand_dims(caption_lengths, 1) < tf.cumsum(ones, 1)\n",
        "\n",
        "        caption_embeddings = self.embedding(caption_tokens)\n",
        "\n",
        "        unidirectional_mask = self._generate_future_mask(max_caption_length)\n",
        "\n",
        "        print(\"cap_vis_mask:\", caption_embeddings.shape, visual_features.shape, unidirectional_mask.shape)\n",
        "\n",
        "        caption_embeddings = tf.transpose(caption_embeddings, [1, 0, 2])\n",
        "        visual_features = tf.transpose(visual_features, [1, 0, 2])\n",
        "        print(caption_embeddings.shape, visual_features.shape)\n",
        "\n",
        "        textual_features = self.encoder(caption_embeddings,\n",
        "                                        visual_features,\n",
        "                                        look_ahead_mask=unidirectional_mask, \n",
        "                                        padding_mask=caption_mask)\n",
        "        textual_features = tf.transpose(textual_features, [1, 0, 2])\n",
        "        op = self.outputL(textual_features)\n",
        "\n",
        "        return op\n",
        "\n",
        "    def  _generate_future_mask(self, size:int):\n",
        "\n",
        "        mask = tf.linalg.band_part(\n",
        "            tf.ones([size, size]), 0, -1\n",
        "        )\n",
        "        return mask\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smpZJkta3rEc",
        "colab_type": "text"
      },
      "source": [
        "## Forward Pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEwqDfp1i7k9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CaptioningModel(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, \n",
        "                 tokenizer, \n",
        "                 textual_head: TransformerTextualHead,\n",
        "                 visual_backbone,\n",
        "                 max_caption_length: int = 30, \n",
        "                 rate:float = 0.0,\n",
        "                ):\n",
        "        super(CaptioningModel, self).__init__()\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.visual_backbone = visual_backbone\n",
        "        self.max_caption_length = max_caption_length\n",
        "        self.textual_head = textual_head\n",
        "        self.visual_projection = L.Dense(self.textual_head.textual_feature_size)\n",
        "\n",
        "    def call(self, images, descriptions):\n",
        "\n",
        "\n",
        "        ps = self.visual_backbone(img)\n",
        "        enc = tokenizer.batch_encode_plus(descriptions, max_length=self.max_caption_length,truncation=True, \n",
        "                                          pad_to_max_length=True,  return_tensors=\"tf\")[\"input_ids\"]\n",
        "\n",
        "        #enc = tf.cast(enc, tf.int64)\n",
        "        #print(enc)\n",
        "        \n",
        "        #(batch_size, height, width, n_channel) -> (batch_size, n_channel, height, width)\n",
        "        ps = tf.transpose(ps, [0, 3, 1, 2]); ps.shape\n",
        "\n",
        "        #(batch_size, n_channel, height, width) -> (batch_size, n_channel, height * width) \n",
        "        rs = tf.reshape(ps, [ps.shape[0], ps.shape[1], -1])\n",
        "\n",
        "        #(batch_size, n_channel, height * width) -> (batch_size, height * width, n_channel)\n",
        "        rs = tf.transpose(rs, [0, 2, 1]); rs.shape\n",
        "\n",
        "        #(batch_size, n_channel, height * width) -> (batch_size, height * width, visual features)\n",
        "        projected = self.visual_projection(rs)\n",
        "\n",
        "        caps = trf(enc, tf.fill((64), 50), projected)\n",
        "\n",
        "        preds_tokens = tf.argmax(caps, 2); preds_tokens.shape\n",
        "\n",
        "        return preds_tokens\n",
        "\n",
        "        \n",
        "\n"
      ],
      "execution_count": 283,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g9kHrG8IH0YP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import GPT2TokenizerFast, BertTokenizerFast\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(\"distilgpt2\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHiHhgU0B2kA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yZs72mcorsa-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrSFDUbGrsXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_NpzoAvFRop",
        "colab_type": "text"
      },
      "source": [
        "### Data prep"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-2MFeq3VGW_T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\n",
        "!wget https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L4I1z15tKQ_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!unzip /content/Flickr8k_Dataset.zip\n",
        "!unzip /content/Flickr8k_text.zip -d Flickr8k_text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFjBrtWiKQ84",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "\n",
        "# load doc into memory\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "# extract descriptions for images\n",
        "def load_descriptions(doc):\n",
        "\tmapping = dict()\n",
        "\t# process lines\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# split line by white space\n",
        "\t\ttokens = line.split()\n",
        "\t\tif len(line) < 2:\n",
        "\t\t\tcontinue\n",
        "\t\t# take the first token as the image id, the rest as the description\n",
        "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
        "\t\t# remove filename from image id\n",
        "\t\timage_id = image_id.split('.')[0]\n",
        "\t\t# convert description tokens back to string\n",
        "\t\timage_desc = ' '.join(image_desc)\n",
        "\t\t# store the first description for each image\n",
        "\t\tif image_id not in mapping:\n",
        "\t\t\tmapping[image_id] = image_desc\n",
        "\treturn mapping\n",
        "\n",
        "def clean_descriptions(descriptions):\n",
        "\t# prepare translation table for removing punctuation\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor key, desc in descriptions.items():\n",
        "\t\t# tokenize\n",
        "\t\tdesc = desc.split()\n",
        "\t\t# convert to lower case\n",
        "\t\tdesc = [word.lower() for word in desc]\n",
        "\t\t# remove punctuation from each token\n",
        "\t\tdesc = [w.translate(table) for w in desc]\n",
        "\t\t# remove hanging 's' and 'a'\n",
        "\t\tdesc = [word for word in desc if len(word)>1]\n",
        "\t\t# store as string\n",
        "\t\tdescriptions[key] =  ' '.join(desc)\n",
        "\n",
        "# save descriptions to file, one per line\n",
        "def save_doc(descriptions, filename):\n",
        "\tlines = list()\n",
        "\tfor key, desc in descriptions.items():\n",
        "\t\tlines.append(key + ' ' + desc)\n",
        "\tdata = '\\n'.join(lines)\n",
        "\tfile = open(filename, 'w')\n",
        "\tfile.write(data)\n",
        "\tfile.close()\n",
        "\n",
        "filename = 'Flickr8k_text/Flickr8k.token.txt'\n",
        "# load descriptions\n",
        "doc = load_doc(filename)\n",
        "# parse descriptions\n",
        "descriptions = load_descriptions(doc)\n",
        "print('Loaded: %d ' % len(descriptions))\n",
        "# clean descriptions\n",
        "clean_descriptions(descriptions)\n",
        "# summarize vocabulary\n",
        "all_tokens = ' '.join(descriptions.values()).split()\n",
        "vocabulary = set(all_tokens)\n",
        "print('Vocabulary Size: %d' % len(vocabulary))\n",
        "# save descriptions\n",
        "save_doc(descriptions, 'descriptions.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4RFd2w_LwSS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!du -shc /content/Flicker8k_Dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9rVm2Cr0MrAJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm.auto import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81WAqLX3LmLv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from os import listdir\n",
        "from pickle import dump\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "# extract features from each photo in the directory\n",
        "def extract_features(directory):\n",
        "\t# load the model\n",
        "\tmodel = ResNet50(input_shape=(224, 224, 3), include_top=False, weights=\"imagenet\")\n",
        "\tprint(model.summary())\n",
        "\t# extract features from each photo\n",
        "\tfeatures = dict()\n",
        "\tfor name in tqdm(listdir(directory)):\n",
        "\t\t# load an image from file\n",
        "\t\tfilename = directory + '/' + name\n",
        "\t\timage = load_img(filename, target_size=(224, 224))\n",
        "\t\t# convert the image pixels to a numpy array\n",
        "\t\timage = img_to_array(image)\n",
        "\t\t# reshape data for the model\n",
        "\t\timage = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
        "\t\t# prepare the image for the VGG model\n",
        "\t\timage = preprocess_input(image)\n",
        "\t\t# get features\n",
        "\t\tfeature = model.predict(image, verbose=0)\n",
        "\t\t# get image id\n",
        "\t\timage_id = name.split('.')[0]\n",
        "\t\t# store feature\n",
        "\t\tfeatures[image_id] = feature\n",
        "\t\t#print('>%s' % name)\n",
        "\treturn features\n",
        "\n",
        "# extract features from all images\n",
        "directory = 'Flicker8k_Dataset'\n",
        "features = extract_features(directory)\n",
        "print('Extracted Features: %d' % len(features))\n",
        "# save to file\n",
        "dump(features, open('features.pkl', 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHauX7uDAxQU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbTl9YxYsKYp",
        "colab_type": "text"
      },
      "source": [
        "### Data Loading and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0EuAhFzUd1t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "3f325c9e-8eee-4879-daa6-e7d7c27d72c4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0e3k1HtCCHNM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wc6rZ7oGCHJI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 274
        },
        "outputId": "95d59006-4aff-41be-8952-a8ccd25e31ea"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv('/content/drive/My Drive/ImageCaption/descriptions.txt', names=['id'])\n",
        "df[['id','caption']] = df[\"id\"].str.split(\" \", 1, expand=True)\n",
        "df[\"caption\"] = df[\"caption\"].str.strip(\"-\")\n",
        "print(df)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                         id                                            caption\n",
            "0     1000268201_693b08cb0e  child in pink dress is climbing up set of stai...\n",
            "1     1001773457_577c3a7d70             black dog and spotted dog are fighting\n",
            "2     1002674143_1b742ab4b8  little girl covered in paint sits in front of ...\n",
            "3     1003163366_44323f5815        man lays on bench while his dog sits by him\n",
            "4     1007129816_e794419615         man in an orange hat starring at something\n",
            "...                     ...                                                ...\n",
            "8087   990890291_afc72be141    man does wheelie on his bicycle on the sidewalk\n",
            "8088    99171998_7cc800ceef             group is sitting around snowy crevasse\n",
            "8089    99679241_adc853a5c0  grey bird stands majestically on beach while w...\n",
            "8090   997338199_7343367d7f                    person stands near golden walls\n",
            "8091   997722733_0cb5439472                 man in pink shirt climbs rock face\n",
            "\n",
            "[8092 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hLNQYGLvHs9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F81sIdqTHuLU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "fdc4ce8e-6eda-4c90-9817-ac2ec8f2a678"
      },
      "source": [
        "imgs=os.listdir(\"/content/drive/My Drive/ImageCaption/Flicker8k_Dataset\"); imgs[:10]"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['378170167_9b5119d918.jpg',\n",
              " '378453580_21d688748e.jpg',\n",
              " '379006645_b9a2886b51.jpg',\n",
              " '380034515_4fbdfa6b26.jpg',\n",
              " '380041023_0dfd712ef1.jpg',\n",
              " '380515798_c2abbf46b0.jpg',\n",
              " '380527679_574749123d.jpg',\n",
              " '380537190_11d6c0a412.jpg',\n",
              " '380590140_25b9889772.jpg',\n",
              " '381052465_722e00807b.jpg']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U02czrgjKwJF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "5a91345f-d98f-4a58-dbe2-58cd4f268f7f"
      },
      "source": [
        "!pip install lycon"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting lycon\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/83/90/b3ff2cdd02dbb0a4ae25f77d6fe8ed8012e7896e59ed59eec78ccf9a92ad/lycon-0.2.0.tar.gz (129kB)\n",
            "\r\u001b[K     |██▌                             | 10kB 27.6MB/s eta 0:00:01\r\u001b[K     |█████                           | 20kB 2.9MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 30kB 3.9MB/s eta 0:00:01\r\u001b[K     |██████████                      | 40kB 4.3MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 51kB 3.4MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 61kB 3.9MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 71kB 4.2MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 81kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 92kB 4.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 102kB 4.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 112kB 4.6MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 122kB 4.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 4.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from lycon) (1.18.5)\n",
            "Building wheels for collected packages: lycon\n",
            "  Building wheel for lycon (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lycon: filename=lycon-0.2.0-cp36-cp36m-linux_x86_64.whl size=740009 sha256=33bf976b4d426d97cb5b8200ae81fa055c99190771dd47761c5c01aa97e811d1\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/a6/03/a13b2750941308b3c4dab5bb88ffb2933bdca7b10f1ab76aa6\n",
            "Successfully built lycon\n",
            "Installing collected packages: lycon\n",
            "Successfully installed lycon-0.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCPQfi8BKyZO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import lycon"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t36hBA1TUUdH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_sample(df: pd.DataFrame, img_dir: str=None, batch_size=64):\n",
        "    len_df = len(df)\n",
        "    print(len_df)\n",
        "\n",
        "    batch_img = []\n",
        "    batch_desc = []\n",
        "    for i in range(batch_size):\n",
        "        idx = np.random.randint(0, len_df, 1)\n",
        "        print(idx)\n",
        "\n",
        "        img_id = df.iloc[idx, 0].values[0]\n",
        "        img_desc = df.iloc[idx, 1].values[0]\n",
        "\n",
        "        print(img_id, img_desc)\n",
        "\n",
        "        img_path = os.path.join(img_dir, img_id+\".jpg\")\n",
        "        \n",
        "        img = load_img(img_path)\n",
        "        img = img.resize([224, 224])\n",
        "\n",
        "        img = img_to_array(img)\n",
        "        img /= 255.0\n",
        "\n",
        "        batch_img.append(img)\n",
        "        batch_desc.append(img_desc)\n",
        "\n",
        "    batch_img = np.array(batch_img)\n",
        "\n",
        "    return batch_img, batch_desc"
      ],
      "execution_count": 194,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ttBVe_uUUZd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c9f8d6b8-2b2b-4d8d-f5fb-12dff69ec88d"
      },
      "source": [
        "%%time\n",
        "img, desc = get_sample(df, \"/content/drive/My Drive/ImageCaption/Flicker8k_Dataset\")"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "8092\n",
            "[1319]\n",
            "2250555512_71670078f5 dark haired man in his twenties drinks green liquid from plastic mug\n",
            "[102]\n",
            "111497985_38e9f88856 kid rock climbing against the backdrop of green valley\n",
            "[6016]\n",
            "3514184232_b336414040 baseball player is making play nearby large sign and boundary of the field\n",
            "[5181]\n",
            "3346289227_198fced308 male on rollerblades skating down railing next to stairs\n",
            "[2407]\n",
            "2603690144_7a28b1d13c boy and dog are running down hill away from crowd\n",
            "[4993]\n",
            "3308018795_68a97a425c group of bike riders are riding on the street\n",
            "[6284]\n",
            "3562282690_cd2a95fe9e black and white dog running on the beach while man stands behind it\n",
            "[4131]\n",
            "3124838157_7ef96745b7 man and two women pose outside retail store\n",
            "[6986]\n",
            "3713177334_32f3245fd8 group of people gathered around mural in an urban area\n",
            "[2944]\n",
            "2788652511_4f10060e07 boy with an orange shirt lies on bodyboard in the surf\n",
            "[7730]\n",
            "549520317_af3d5c32eb child in red jacket sitting atop slide\n",
            "[3781]\n",
            "3036971334_78187a9570 blond man jumping off cliff into some water\n",
            "[3775]\n",
            "3035949542_cb249790f5 the legs of two athletes playing soccer on field\n",
            "[5038]\n",
            "3320209694_db579cb607 man in purple had climbs rocky wall with his hands\n",
            "[2461]\n",
            "2623930900_b9df917b82 man who has gray beard and gray hair laughs while wearing purple shirt\n",
            "[6560]\n",
            "3619416477_9d18580a14 white dog catches green ball while standing in shallow water\n",
            "[5304]\n",
            "3367758711_a8c09607ac there are two women crossing the street in front of two buses\n",
            "[7326]\n",
            "448252603_7d928c900e boy pointing in direction on dirt road\n",
            "[1358]\n",
            "2261550615_b6c25d987b man in cowboy hat is walking through market and reaching into his pocket\n",
            "[1139]\n",
            "2178095150_436b035741 dogs participating in race\n",
            "[513]\n",
            "1469358746_2a879abaf3 man and his dog in the mountains\n",
            "[5678]\n",
            "3443161359_65544fd732 male hiker is standing on top of very large rock and is taking pictures\n",
            "[3486]\n",
            "2945036454_280fa5b29f dog is jumping in the air trying to catch red frisbee\n",
            "[5187]\n",
            "3347666612_659e6e2207 children with painted red faces being sprayed with water on grass\n",
            "[4496]\n",
            "3208188198_2b271d2a2e person jogging on sidewalk\n",
            "[535]\n",
            "1479679558_d0a01bc62b creek winds through the woods\n",
            "[1003]\n",
            "2100735137_05c6079537 little boy jumps on toy air gun\n",
            "[1301]\n",
            "2244613488_4d1f9edb33 four bicyclists ride along dirt road between two wire fences\n",
            "[7398]\n",
            "469969326_4b84073286 woman carrying backpack sits on rocky ledge overlooking the water\n",
            "[6668]\n",
            "3640329164_20cb245fd5 surfboarder catches the waves\n",
            "[467]\n",
            "1439046601_cf110a75a7 boy is smiling whilst standing in front of swimming pool\n",
            "[1280]\n",
            "223299137_b0e81ac145 beautiful sunset with three people in boat on the lake\n",
            "[7285]\n",
            "437054333_5c2761b8cd bus filled with passengers in chicago at night\n",
            "[4141]\n",
            "3126773489_7ae425af17 basketball player defends another player\n",
            "[3049]\n",
            "2832076014_ff08c92037 car is in the water\n",
            "[5228]\n",
            "3353278454_2f3a4d0bbc small dog walks with men in the city\n",
            "[2367]\n",
            "2594459477_8ca0121a9a little girl in rain puddle\n",
            "[3752]\n",
            "3029715635_43ab414dfb black and white dog with blue toy in its mouth runs on grassy surface just beyond fence\n",
            "[48]\n",
            "106490881_5a2dd9b7bd boy in his blue swim shorts at the beach\n",
            "[4934]\n",
            "3293753378_7a8ddb98b2 fashionably dressed woman holding record in frame at used bookstore\n",
            "[357]\n",
            "1370615506_2b96105ca3 wine shop with an open sign in the window and two people out front\n",
            "[3609]\n",
            "2990977776_1ec51c9281 boy is doing stunt with his skateboard\n",
            "[571]\n",
            "1522787272_5a31497ef2 dog with mud stuck on his underside is running on grass\n",
            "[4312]\n",
            "3168841415_c0705a327a boy in brown shirt throwing frisbee\n",
            "[2058]\n",
            "2484190118_e89363c465 boy with his hands above his head stands on cement pillar above the cobblestones\n",
            "[5016]\n",
            "3315616181_15dd137e27 man in green shirt climbs an indoor climbing wall\n",
            "[1646]\n",
            "2367317953_503317493e female soccer player gives teammate piggyback ride\n",
            "[1215]\n",
            "2204695848_3d1b140212 blond girl earring groucho marx glasses\n",
            "[3490]\n",
            "2947172114_b591f84163 group of little boys play football in uniforms\n",
            "[2327]\n",
            "2579268572_d78f8436cb group of people riding roller coaster upside down\n",
            "[1426]\n",
            "2282260240_55387258de little girl dances in room with balloons on the floor\n",
            "[7098]\n",
            "380537190_11d6c0a412 group of people stand at farmers market on dreary day\n",
            "[5805]\n",
            "3470129475_9e58b6742c boy does trick on his bike as other kids watch\n",
            "[6854]\n",
            "3682038869_585075b5ff little girl is peaking out from behind tree\n",
            "[718]\n",
            "1799271536_6e69c8f1dc brown and black dog runs down sandy beach\n",
            "[6859]\n",
            "3683592946_262e9bfbfd bird is standing on rock next to her six babies\n",
            "[5580]\n",
            "3426964258_67a0cee201 crowd of people are gathered in the street waving irish flags\n",
            "[8043]\n",
            "952171414_2db16f846f group of four people conversing next to bus\n",
            "[3211]\n",
            "2874728371_ccd6db87f3 large brown dog and small black dog stand near each other on dirt patch surrounding by grass and weeds\n",
            "[5085]\n",
            "3329289652_e09b80e2f3 the boys are playing with legos\n",
            "[3858]\n",
            "3054200086_657d4398e8 hockey player makes shot\n",
            "[5984]\n",
            "3504881781_6a842e043b group of men are changing the shells on cannon\n",
            "[7109]\n",
            "384465370_9918873f9a two cyclists atop hill as seen from below\n",
            "[5333]\n",
            "3374054694_fa56f29267 brown dog is jumping up at woman in black coat\n",
            "CPU times: user 642 ms, sys: 37.3 ms, total: 679 ms\n",
            "Wall time: 15.6 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tE78BB9gHrZq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZOVdpVTsHrXC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUE1c9R0sOIx",
        "colab_type": "text"
      },
      "source": [
        "### Forward Pass"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaXJpmIzi7h6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cap = CaptioningModel(tokenizer, trf, visual_backbone, max_caption_length=50)"
      ],
      "execution_count": 293,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGM0EDslH-u9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trf = TransformerTextualHead(vocab_size=50258, hidden_size=256, num_layers=6, attention_heads=8, feedforward_size=256, max_caption_length=50)"
      ],
      "execution_count": 294,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MYU6w73gi7ft",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 697
        },
        "outputId": "afadc5ef-52f6-4bf7-a1e9-2e05119273c8"
      },
      "source": [
        "%%time\n",
        "caps = cap(img, desc)"
      ],
      "execution_count": 297,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50\n",
            "(64, 50)\n",
            "(64, 50)\n",
            "cap_vis_mask: (64, 50, 256) (64, 49, 256) (50, 50)\n",
            "(50, 64, 256) (49, 64, 256)\n",
            "TGT:(64, 50, 256)\n",
            "Q:(64, 8, 50, 32), K:(64, 8, 50, 32), V:(64, 8, 50, 32)\n",
            "TGT:(64, 50, 256), ENC: (64, 49, 256)\n",
            "Q:(64, 8, 50, 32), K:(64, 8, 49, 32), V:(64, 8, 49, 32)\n",
            "target: (64, 50, 256), enc_op:(64, 49, 256)\n",
            "TGT:(64, 50, 256)\n",
            "Q:(64, 8, 50, 32), K:(64, 8, 50, 32), V:(64, 8, 50, 32)\n",
            "TGT:(64, 50, 256), ENC: (64, 49, 256)\n",
            "Q:(64, 8, 50, 32), K:(64, 8, 49, 32), V:(64, 8, 49, 32)\n",
            "target: (64, 50, 256), enc_op:(64, 49, 256)\n",
            "TGT:(64, 50, 256)\n",
            "Q:(64, 8, 50, 32), K:(64, 8, 50, 32), V:(64, 8, 50, 32)\n",
            "TGT:(64, 50, 256), ENC: (64, 49, 256)\n",
            "Q:(64, 8, 50, 32), K:(64, 8, 49, 32), V:(64, 8, 49, 32)\n",
            "target: (64, 50, 256), enc_op:(64, 49, 256)\n",
            "TGT:(64, 50, 256)\n",
            "Q:(64, 8, 50, 32), K:(64, 8, 50, 32), V:(64, 8, 50, 32)\n",
            "TGT:(64, 50, 256), ENC: (64, 49, 256)\n",
            "Q:(64, 8, 50, 32), K:(64, 8, 49, 32), V:(64, 8, 49, 32)\n",
            "target: (64, 50, 256), enc_op:(64, 49, 256)\n",
            "TGT:(64, 50, 256)\n",
            "Q:(64, 8, 50, 32), K:(64, 8, 50, 32), V:(64, 8, 50, 32)\n",
            "TGT:(64, 50, 256), ENC: (64, 49, 256)\n",
            "Q:(64, 8, 50, 32), K:(64, 8, 49, 32), V:(64, 8, 49, 32)\n",
            "target: (64, 50, 256), enc_op:(64, 49, 256)\n",
            "TGT:(64, 50, 256)\n",
            "Q:(64, 8, 50, 32), K:(64, 8, 50, 32), V:(64, 8, 50, 32)\n",
            "TGT:(64, 50, 256), ENC: (64, 49, 256)\n",
            "Q:(64, 8, 50, 32), K:(64, 8, 49, 32), V:(64, 8, 49, 32)\n",
            "target: (64, 50, 256), enc_op:(64, 49, 256)\n",
            "CPU times: user 193 ms, sys: 10.9 ms, total: 204 ms\n",
            "Wall time: 234 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l8um-QoWHrSH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}